Linear Regression ‚Äî Algorithm 1 of 14 (IIT Kanpur AML)
This project demonstrates Linear Regression ‚Äî a foundational supervised learning algorithm ‚Äî as part of the Applied Machine Learning (AML) course from IIT Kanpur. It explains the intuition, real-world use, step-by-step math, training logic, and model evaluation in a clear and beginner-friendly format.
________________________________________
1. Definition
Linear Regression is a supervised learning algorithm used to model the relationship between a dependent variable (Y) and one or more independent variables (X) using a straight line.
 It is used to predict continuous numerical outcomes.
________________________________________
2. Why Do We Use It?
To predict continuous outcomes like:
‚Ä¢	 House prices based on area, location, and number of rooms.
‚Ä¢	Salary based on years of experience.
‚Ä¢	Sales forecasting based on advertising spend.
‚Ä¢   Health metrics like blood pressure based on age, weight.
________________________________________
 3. Real-World Analogy
Imagine you're a tailor. You charge ‚Çπ100 per meter of cloth.
‚Ä¢	1 meter ‚Üí ‚Çπ100
‚Ä¢	2 meters ‚Üí ‚Çπ200
‚Ä¢	3 meters ‚Üí ‚Çπ300
You‚Äôll notice a clear linear pattern:
"As the cloth length increases, the price increases proportionally."
So, for 4.5 meters ‚Üí You can predict ‚Çπ450.
That‚Äôs Linear Regression in action!
________________________________________
 4. What is the Algorithm Trying to Predict?
Linear Regression tries to learn a mapping function from X to Y:
Y = mX + c
Example:
‚Ä¢	X = Hours studied
‚Ä¢	Y = Exam Score
‚Ä¢	Model might learn: Y = 10X + 5
Meaning:
‚Ä¢	Every additional hour studied increases marks by 10
‚Ä¢	Base score (intercept) is 5
________________________________________
5. How Does It Work? (Step-by-Step)
1.	You provide training data: known X and Y
2.	Model starts with random values of m and c
3.	Predicts: Y_pred = mX + c
4.	Calculates error using loss function (e.g., MSE)
5.	Uses Gradient Descent to adjust m and c
6.	Repeats steps 3‚Äì5 until error is minimized
________________________________________
6. Math Behind It (Formulas + Terms)
 Cost Function ‚Äî Mean Squared Error (MSE)

MSE = (1/n) * Œ£(Y_actual - Y_pred)^2
Where:
‚Ä¢	n = number of data points
‚Ä¢	Y_actual = real value
‚Ä¢	Y_pred = predicted value

Gradient Descent (to minimize MSE)
m = m - Œ± * ‚àÇ(MSE)/‚àÇm
c = c - Œ± * ‚àÇ(MSE)/‚àÇc
Where:
‚Ä¢	Œ± = learning rate
‚Ä¢	‚àÇ = partial derivative (gradient)
‚Ä¢	Repeats until slope of error ‚Üí 0
________________________________________
 7. What is Training vs Prediction?
 Training:
You feed input-output data ‚Üí Model learns best m and c
 Prediction:
You give new X ‚Üí Model calculates Y = mX + c
Flow:
Training:
(X, Y) ‚Üí Learn m, c ‚Üí Model

Prediction:
New X ‚Üí Predict Y using mX + c
________________________________________
8. What Do We Get After Training?
‚Ä¢	Final equation: Y = mX + c
‚Ä¢	A trained model that generalizes to new data
‚Ä¢	Evaluation metrics like:
o	R¬≤ Score
o	Mean Squared Error
o	Root Mean Squared Error (RMSE)
________________________________________
9. Purpose of Training
‚Ä¢	Find the best-fit line that minimizes error
‚Ä¢	Capture patterns from past data
‚Ä¢	Enable accurate prediction on unseen data
‚Ä¢	Quantify how features impact the output
________________________________________
10. What Can We Measure?
Metric	Description
MSE	Average of squared prediction errors
RMSE	Square root of MSE
R¬≤ Score	Measures how well the line fits the data
Coefficients	Slope values (impact of each feature on output)
________________________________________
11. Where Can We Use It in Real Life?
Domain	Use Case
Finance	Predicting stock prices
Health	Estimating patient vitals like BP/sugar
Marketing	Forecasting ad revenue
Education	Predicting student exam scores
Retail	Sales prediction based on pricing
________________________________________
 12. Summary: How Model Learns m and c
Give Input data (X and Y)
and Guess Start with random m, c post that it will
Predict	Compute Y = mX + c and 
Compare	Use loss function to check error and after that it will
Adjust	Apply gradient descent and finally
Repeat	Until loss is minimized
________________________________________
Quick Example
Given:
X = [1, 2, 3, 4]
Y = [50, 55, 65, 70]
1.	Model starts with: Y = X ‚Üí Predicts Y = [1, 2, 3, 4]
2.	Loss is large ‚Üí Adjust m and c
3.	After training: Y = 6.25X + 43.5 ‚Üí Better predictions
________________________________________
Final Notes
‚Ä¢	Linear Regression is your go-to when predicting a quantity.
‚Ä¢	It forms the base for more advanced algorithms.
‚Ä¢	Always visualize your data and line to ensure proper fit.
________________________________________
üèÜ Author
Krishna Teja Reddy Lingala
IIT Kanpur Certified | Applied Machine Learning


